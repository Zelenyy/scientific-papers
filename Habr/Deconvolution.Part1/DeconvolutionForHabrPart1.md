# Статистическая регуляризация некорректных обратных задач им. Турчина. Часть 1

Привет, Хабр.

Сегодня мы хоти представить пост от одной из входящих dJetBrains Researh лабораторий, а именно лаборатории методики ядерно-физического эксперимента.
Где JetBrains  и где ядерная физика спросите вы? Ну мы сошлись почве любви к Kotlin, хотя в данном посте о нем речи идти не будет. Дело во вполне объективных причинах, современный уровень финансирования Российской фундаментальной науки, мести оставляет желать лучшего и ка следствие многим исследователям приходиться акцентировать внимание на более дешевых исследованиях, в частности наша группа ориентируется на развитие методик анализа данных, моделирования и написание софта для ученных.

В этой статье мы как раз хотим поговорить об одном методе популяризуемом нашей группой: *статистической регуляризации*, созданного В.Ф. Турчинным в 70-х годах XX века, и реализации в виде кода на Python и Julia.


Оглавление (изложение будет достаточно подробным, поэтому для тех кому все очевидно с обратными задачами, могут сразу переходить к примера, а теорию прочитать в [этой статье]()):

* Возникновение проблемы: зачем вообще кого-то регуляризировать?
* Теоретическое описание статистической регуляризации
* Практические примеры на:
  * Python
  * Julia
* Заключение


## Возникновение проблемы: зачем вообще кого-то регуляризировать?

Если достаточно абстрагироваться, то любое измерение в эксперименте можно описать следующим образом: у нас есть некий прибор, которые фиксирует спектр или сигнал какого-либо процесса, и по результатам измерения показывает на какие-то цифры, наша задача как исследователей глядя на эти цифры и зная устройство прибора, понять а какой был измеряемый спектр или сигнал. То есть на лицо, то что называется *обратной задачей*. Если мы попробуем прелставить это математически, то получим вот такое уравнение (которое, кстати называется уравнением Фредгольма I рода):
$$
f(y) = \int \limits_a^b dx K(x,y)\varphi(x)
$$
Фактически, это уравнение описывает следующее: наш измерительный прибор представлен здесь своей аппаратной функцией $K(x,y)$, которая действует на исследуемый спектр или иной входной сигнал $\varphi$, в результате чего исследователь наблюдает выходной сигнал $f(y)$. Целью исследователя является востановить сигнал $\varphi$ по известным $f(y)$ и $K(x,y)$. Так же можно сформилировать это выражение в матричной форме, заменив функции векторами и матрицами:
 $$
 f_m = K_{mn}\varphi_n
 $$
Казалось бы, восстановление сигнала не является сложной
задачей, поскольку что уравнение Фредгольма, что система линейный уравнений (даже переопределенная) имеют точное решение. 
Так давайте попробуем, пусть измеряемы сигнал описывается как сумма двух гауссов:
$$
\varphi(x) = 2*N(2, 0.16) + N(4, 0.04)
$$
В качестве прибора мы возьмем простейший интегратор --- матрицу переводящую наш сигнал в кумулятивную сумму  с помощи функции Хевисайда:
$$
K_{mn} = \theta(x_m-y_n)
$$
Вид измеряемого сигнал и нашего прибора, а так же результат измерения приведены на графике. Важным моментом является, то что всякое реальное измерение имеет ошибки измерения, поэтому мы немного испортим наш результат измерения, добавив нормальный шум, дающий пятипроцентную ошибку измерения.
Восстанавливать сигнал мы будем методом наименьших квадратов:
$$
\varphi^{МНК} = (K^TK)^{-1}K^Tf
$$
И в результате мы получим:

Собственно на этом и можно было закончить статью, в очередной раз убедившись в бесполезности идеалистических методов математики перед суровой и безжалостной физической реальностью, и пойти раскуривать паяльники.
Но давайте сначала разберемся из-за чего сия неудача постигла нас? Очевидно дело в ошибках измерений, но что же они влияют? Дело в том что еще  Жак Адамар (тот самый, который добавил черточку в формулу Коши-Адамара) разделий все задачи на [корректно поставленные](https://ru.wikipedia.org/wiki/%D0%9A%D0%BE%D1%80%D1%80%D0%B5%D0%BA%D1%82%D0%BD%D0%BE_%D0%BF%D0%BE%D1%81%D1%82%D0%B0%D0%B2%D0%BB%D0%B5%D0%BD%D0%BD%D0%B0%D1%8F_%D0%B7%D0%B0%D0%B4%D0%B0%D1%87%D0%B0) и некорректно. Вспомнив классиков: " Бессмыслица — искать решение, если оно и так есть. Речь идёт о том, как поступать с задачей, которая решения не имеет. Это глубоко принципиальный вопрос… ", мы не будет говорит о корректных задачах и сразу возьмемся за некорректные, благо мы уже выше встретили  такую: написанное нами выше уравнение Фредгольма является не корректной обратной задачей --- *даже при бесконечно малых флюктуациях во входных данных (а уж наши ошибки измерения далеко не бесконечно малые), решение уравнения полученное точным аналитическим образом может сколь угодно отличаться от истинного*. Доказательство этого утверждения вы можете прочитать в классическом труде академика А. Н Тихонова "Методы решения некорректных задач" в первой главе (оно хоть и требует некоторых познаний, но достаточно просто воспринимается). Как вы можете догадаться в этой книге есть советы что собственно делать с некорректными задачи, однако изложенная там методика имеет ряд недостатков, которые устранены в методе Турчина. Но для начала изложим общие принципы работы с некорректными задачи, что же надо делать если вам попалась такая задача? Поскольку сама задача не может нам ничего предложить, нам приходиться пойти на небольшое преступление: нужно дополнить задачу данными так, что бы она стала корректной или иначе говоря мы должны ввести какую-то *дополнительныею априоную информацию о задаче*.


Но
уравение Фредгольма некорректно - бесконечно малое изменение начальных
условий приводит к конечному изменению решения. Таким образом, наличие
шумов, присутсвущих в любом эксперименте, обесценивает попытки решить
это уравение **точно**.

## Теория

Расмотрим некую алгебраизацию уравнения Фредгольма: С точки зрения математической
статистики мы должны должны оценить $\vec{\varphi}$ по реализации $\vec{f}$, зная плотность
вероятности для $\vec{f}$ и содержимое матрицы
$K$. Действуя в духе теории принятия решений, мы
должны выбрать вектор-функцию $(\vec{S}$,
определяющую $\vec{\varphi}$ на основе
$\vec{f}$ и называемую *стратегией*. Для того,
чтобы определить, какие стратегии более оптимальные, мы введем
*квадратичную функцию потерь*: 
$$ 
L(\hat{\varphi},\vec{S}) =
(\hat{\varphi}-\vec{S})^2,
$$ 
где $\hat{\varphi}$ — наилучшее решение. Согласно
баейсовскому подходу рассмотрим $\vec{\varphi}$
как **случайную переменную** и переместим нашу неопределенность о
$\vec{\varphi}$ в *априорную плотность*
$P(\vec{\varphi})$, выражающую **достоверность**
различных возможных законов природы и определяемую на основе информации,
сущетсвующей до проведения эксперимента. При таком подходе выбор
оптимальной стратегии основывается на минимизации *апостериорного
риска*: 
$$
r_{\vec{S}}(\vec{\varphi}) \equiv
E_{\vec{\varphi}}E_{\vec{f}}L(\vec{\varphi},\vec{S})|\vec{\varphi}
$$ 
Тогда оптимальная стратегия в случае квадратичной функции
потерь хорошо изветсна: 
$$
S^{opt} _n= E[\varphi_n|\vec{f}] =
\int \varphi_n P(\vec{\varphi}|\vec{f})d\vec{\varphi}
$$
*Апостерионая плотность*
$(P(\vec{\varphi}|\vec{f})$ определяется по
теореме Баейса: 
$$
P(\vec{\varphi}|\vec{f})=
\frac{P(\vec{\varphi})P(\vec{f}|\vec{\varphi})}{\int
d\vec{\varphi}P(\vec{\varphi})P(\vec{f}|\vec{\varphi})}
$$ 
Кроме того, такой подход позволяет определить дисперсию
полученного решения: 
$$
\left\langle \sigma_n^2 \right\rangle =
\int (\varphi_n - S^{opt}_n)^2
P(\vec{\varphi}|\vec{f})d\vec{\varphi}
$$ 
Мы получили решение, введя априорную плотность
$P(\vec{\varphi})$. Можем ли мы сказать, что-либо
о том мире функций $\varphi(x)$, который задается
априорной плотностью? Если ответ на этот вопрос отрицательный, то мы
должны будем принять все возможные $\varphi(x)$
равновероятными и вернуться к нерегуляризованному решению. Таким
образом, мы должны ответить на этот вопрос положительно. Именно в этом
заключается метод статистической регуляризации — регуляризация решения
за счет введения дополнительной априорной информации о
$\varphi(x)$. Если исследователь уже обладает
какой-либо априорной информацией (априорной плотностью
$P(\vec{\varphi})$), он может просто вычислить
интеграл и получить ответ. В случае, если такой информации нет, в
следующем параграфе описывается, какой минимальной информацией может
обладать исследователь и как её использовать для получения
регулязованного решения.

## Априоная информация

Как показали британские ученые, во всем остальном мире любят
дифференцировать. Причем, если математик будет задаваться вопросами о
правомерности этой операции, то физик оптимистично верит, что законы
природы описываются “хорошим” функциями, то есть гладкими. Иначе говоря,
он назначает более гладким $\varphi(x)$ более
высокую априорную плотность вероятности. Так давайте попробуем ввести
априорную вероятность, основанную на гладкости. Для этого мы вспомним,
что введение априорной информации — это некоторое насилие над миром,
принуждающее законы природы выглядеть удобным для нас образом. Это
насилие следует свести к минимуму, и, вводя априорную плотность
вероятности, необходимо, что бы *информация Шеннона* относительно
$\varphi(x)$, содержащаяся в
$P(\vec{\varphi})$, была минимальной. Формализуя
выше сказанное, выведем вид априорной плотности, основанной на гладкости
функции. Для этого мы будем искать условный экстремум информации:
$$
I[P(\vec{\varphi})] = \int \ln{P(\vec{\varphi})}
P(\vec{\varphi}) d\vec{\varphi} \to min 
$$
При следующих условиях:
1.  Условие на гладкость $\varphi(x)$. Пусть
    $\Omega$ — некоторая матрица, характеризующая
    гладкость функции. Тогда потребуем, чтобы достигалось определённое
    значение функционала гладкости: 
    $$
    \int(\vec{\varphi},\Omega\vec{\varphi}) P(\vec{\varphi})
    d\vec{\varphi} = \omega
    $$ 
    Внимательный
    читатель должен задать вопрос об определении значения параметра
    $\omega$. Ответ на него будет дан далее по
    тексту.

2.  Нормированность вероятности на единицу:
$$
 \int     P(\vec{\varphi}) d\vec{\varphi} = 1
$$
При этих условиях доставлять минимум функционалу будет следующая
функция: 
$$
P_{\alpha}(\vec{\varphi}) =
\frac{\alpha^{Rg(\Omega)/2}\det\Omega^{1/2}}{(2\pi)^{N/2}}
\exp(-\frac{1}{2}
(\vec{\varphi},\alpha\Omega\vec{\varphi}))
$$
Параметр $\alpha$ cвязан с $\omega$, но поскольку у нас нет собственно информации о конкректных
значениях функционала гладкости, выяснять, как имеено он связан,
бессмысленно. Что же тогда делать с $\alpha$,
спросите вы? Здесь перед вами расрываются три пути:
1. подбирать значение параметра $\alpha$ вручную, и тем самым перейти к регуляризации Тихонова,
2. усреднить по всем возможным $\alpha$, предпологая все возможные $\alpha$ равновероятными,
3. выбрать наиболее вероятное $\alpha$ по его апостериорной плотности вероятности $P(\alpha|\vec{f})$. Этот подход верен, если мы прдепологаем, что в экспериментальных данных содержится достаточно ифнормаци об $\alpha$. 

Первый случай нам мало интересен. Во втором случае мы получим следующую формулу для решения: 
$$
\left\langle \varphi_i \right\rangle = \frac{\int
d\varphi\, \varphi_i P(f|\varphi) \int\limits
d\alpha\,P(\alpha) \alpha^{\frac{Rg(\Omega)}{2}}
\exp(-\frac{\alpha}{2}
(\vec{\varphi},\Omega\vec{\varphi}))}{\int d\varphi P(f|\varphi)
\int\limits d\alpha\,P(\alpha) \alpha^{\frac{Rg(\Omega)}{2}}
\exp(-\frac{\alpha}{2}
(\vec{\varphi},\Omega\vec{\varphi}))}
$$
Третий случай будет рассмотрен в следующем разделе на примере гауссовых шумов в эксперименте.

## Случай гауссовых шумов

Случай, когда ошибки в эксперименте распределны по Гауссу, замечателен
тем, что можно получить аналитическое решение нашей задачи. Решение и
его ошибка будут иметь следующий вид:
$$
 \vec{\varphi} = (K^T\Sigma^{-1}K + \alpha^*\Omega)^{-1}K^T\Sigma^{-1^{T}}\vec{f}
$$

$$ 
\Sigma_{\vec{\varphi}} = (K^T\Sigma^{-1}K+\alpha^*\Omega)^{-1}
$$
 где $\Sigma$ - ковариционная матрица многомерного
распределения Гаусса, $\alpha^*$ - наиболее
вероятное значение параметра $\alpha$, которое
определяется из условия максимума апостериорной плотности вероятности:
$$
P(\alpha|\vec{f}) = C'\alpha^{\frac{Rg(\Omega)}{2}}\sqrt{|(K^T\Sigma^{-1}K+\alpha\Omega)^{-1}|}\exp(\frac{1}{2}
\vec{f}^T\Sigma^{-1}K^{T}(K^T\Sigma^{-1}K+\alpha\Omega)^{-1}K^T\Sigma^{-1^{T}}\vec{f})
$$

В качестве примера рассмотрим востановление спектра, состоящего из двух
гауссовых пиков, которые попали под действие интегрального
ядра-ступеньки (функции Хевисайда).

![]($%7Br%20'/images/maths/deconvolution.png'%7D)

## В следующей части

* Примерение MCMC
* Разложение Холецкого
* Применение регуляризации для создание орбитального детектора протонов и электронов

## Ссылки

* []()
* []()
* []()
* []()
* []()
